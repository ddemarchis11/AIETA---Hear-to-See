{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c090c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.makedirs(\"mixed_up_data_speak/speak\", exist_ok = True)\n",
    "\n",
    "VOICE_DIR = os.path.abspath(\"datasets/mathurinache/the-lj-speech-dataset/versions/1/LJSpeech-1.1/wavs\")\n",
    "BG_DIR = os.path.abspath(\"datasets/background_simplified\")\n",
    "OUT_DIR = os.path.abspath(\"mixed_up_data_speak/speak\")\n",
    "AMPLIFICATION = [10, 15, 20]\n",
    "\n",
    "voice_files = glob.glob(os.path.join(VOICE_DIR, \"*.wav\"))\n",
    "bg_noise_type = [\"cafeteria_noises\", \"metro_noises\", \"park_noises\", \"station_noises\", \"traffic_noises\"]\n",
    "\n",
    "def mix_it_up(idx):\n",
    "    for type in bg_noise_type:\n",
    "        \n",
    "        bg_files    = glob.glob(os.path.join(BG_DIR, type, \"*.wav\"))\n",
    "\n",
    "        voice_path = random.choice(voice_files)\n",
    "        bg_path = random.choice(bg_files)\n",
    "\n",
    "        voice = AudioSegment.from_file(voice_path)\n",
    "        background = AudioSegment.from_file(bg_path)\n",
    "\n",
    "        background += random.choice(AMPLIFICATION)\n",
    "\n",
    "        combined = voice.overlay(background)\n",
    "\n",
    "        out_path = os.path.join(OUT_DIR, f\"mixed_noisy_{type}_{idx}.wav\")\n",
    "        combined.export(out_path, format = 'wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38686d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8394a78b0d2c49419b24f93dc58fe376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creazione Speak Set - Parziale:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(20), desc=\"Creazione Speak Set - Parziale\"): mix_it_up(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "175e8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"mixed_up_data_no_talk/noisy\", exist_ok = True)\n",
    "os.makedirs(\"mixed_up_data_no_talk/music\", exist_ok=True)\n",
    "\n",
    "BG_DIR = os.path.abspath(\"datasets/background_simplified\")\n",
    "OUT_DIR_MUSIC = os.path.abspath(\"mixed_up_data_no_talk/music\")\n",
    "OUT_DIR_NOISE = os.path.abspath(\"mixed_up_data_no_talk/noisy\")\n",
    "\n",
    "def augment_file(input_path, output_dir, sample_rate, stretch_factors, pitch_steps, amplification):\n",
    "    \n",
    "    y, sr = librosa.load(input_path, sr=sample_rate)\n",
    "    basename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    \n",
    "    ops = ['stretch', 'pitch', 'gain', 'none']\n",
    "    choice = random.choice(ops)\n",
    "    \n",
    "    if choice == 'stretch':\n",
    "        factor = random.choice(stretch_factors)\n",
    "        y_stretch = librosa.effects.time_stretch(y, rate=factor)\n",
    "        out_name = f\"{basename}_stretch{factor:.2f}.wav\"\n",
    "        sf.write(os.path.join(output_dir, out_name), y_stretch, sr)\n",
    "\n",
    "    if choice == 'pitch':\n",
    "        steps = random.choice(pitch_steps)\n",
    "        y_shift = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=steps)\n",
    "        out_name = f\"{basename}_pitch{steps:+d}.wav\"\n",
    "        sf.write(os.path.join(output_dir, out_name), y_shift, sr)\n",
    "    \n",
    "    if choice == 'gain':\n",
    "        gain = random.choice(amplification)\n",
    "        y_amplify = y * gain\n",
    "        out_name = f\"{basename}_amplified_{gain:.1f}x.wav\"\n",
    "        sf.write(os.path.join(output_dir, out_name), y_amplify, sr)\n",
    "    \n",
    "    if ops == 'none': sf.write(os.path.join(output_dir, basename), y, sr)\n",
    "\n",
    "def batch_augment_music(input_dir, output_dir,\n",
    "                  sample_rate=44100,\n",
    "                  stretch_factors=(0.9, 1.1),\n",
    "                  pitch_steps=(-2, 2), amplification = 0):\n",
    "    \n",
    "    for f in tqdm(os.listdir(input_dir), desc=\"Augumenting Music\"):\n",
    "        base, ext = os.path.splitext(f)\n",
    "        \n",
    "        inp_path = os.path.join(input_dir, f)\n",
    "        augment_file(inp_path, output_dir, sample_rate, stretch_factors, pitch_steps, amplification)\n",
    "        \n",
    "def batch_augment_noise(input_dir, output_dir,\n",
    "                  sample_rate=44100,\n",
    "                  stretch_factors=(0.9, 1.1),\n",
    "                  pitch_steps=(-2, 2), amplification = 0):\n",
    "    \n",
    "    bg_noise_type = [\"cafeteria_noises\", \"metro_noises\", \"park_noises\", \"station_noises\", \"traffic_noises\"]       \n",
    "    \n",
    "    for i in tqdm(bg_noise_type, desc=\"Augumenting Noise\"):\n",
    "        dir_path = os.path.join(input_dir,i)\n",
    "        for f in os.listdir(dir_path):                        \n",
    "            inp_path = os.path.join(dir_path, f)\n",
    "            augment_file(inp_path, output_dir, sample_rate, stretch_factors, pitch_steps, amplification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "969861ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b876a2d6254296a53fdbe736a44c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augumenting Music:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3127e78b5d5d4ae490a350c9e94e4d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augumenting Noise:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2124217150417fa4d4a6d00ae17282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augumenting Music:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50c56502ec64eae9ff37516f660ee51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augumenting Music:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentazione completata!\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"mixed_up_data_speak/no_speak\", exist_ok = True)\n",
    "\n",
    "OUT_DIR_SPEAK = os.path.abspath(\"mixed_up_data_speak/speak\")\n",
    "OUT_DIR_NO_SPEAK = os.path.abspath(\"mixed_up_data_speak/no_speak\")\n",
    "\n",
    "music_path = os.path.abspath(\"datasets/music_set_wav/complete_song\")\n",
    "vocals_path = os.path.abspath(\"datasets/music_set_wav/vocals\")\n",
    "instrumentals_path = os.path.abspath(\"datasets/music_set_wav/instrumentals\")\n",
    "noise_path = os.path.abspath(\"datasets/background_simplified\")\n",
    "\n",
    "sr       = 44100\n",
    "stretches = [0.8, 1.2]\n",
    "pitches  = [-3, 3]\n",
    "amplification = [-5, 1, +5]\n",
    "\n",
    "batch_augment_music(instrumentals_path, OUT_DIR_NO_SPEAK, sr, stretches, pitches, amplification)\n",
    "batch_augment_noise(noise_path, OUT_DIR_NO_SPEAK, sr, stretches, pitches, amplification)\n",
    "batch_augment_music(music_path, OUT_DIR_SPEAK, sr, stretches, pitches, amplification)\n",
    "batch_augment_music(vocals_path, OUT_DIR_SPEAK, sr, stretches, pitches, amplification)\n",
    "\n",
    "print(\"Augmentazione completata!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6b06b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import os\n",
    "import random\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def segment_file(in_path: str, out_dir: str, window_s: float, segments: int) -> None:\n",
    "    \n",
    "    audio = AudioSegment.from_file(in_path)\n",
    "    dur_ms = len(audio)\n",
    "    window_ms = int(window_s * 1000)\n",
    "    base, _ = os.path.splitext(os.path.basename(in_path))\n",
    "\n",
    "    if dur_ms < window_ms:\n",
    "        out_name = f\"{base}segment{1:03d}.wav\"\n",
    "        audio.export(os.path.join(out_dir, out_name), format=\"wav\")\n",
    "        return\n",
    "        \n",
    "    for i in range(segments):\n",
    "        start = random.randint(0, dur_ms - window_ms)\n",
    "        end = start + window_ms\n",
    "        segment = audio[start:end]\n",
    "\n",
    "        out_name = f\"{base}segment{i:03d}.wav\"\n",
    "        segment.export(os.path.join(out_dir, out_name), format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1efdad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_s = 3\n",
    "\n",
    "os.makedirs(\"mixed_up_data_speak_segmented\", exist_ok=True)\n",
    "os.makedirs(\"mixed_up_data_speak_segmented/speak\", exist_ok=True)\n",
    "os.makedirs(\"mixed_up_data_speak_segmented/no_speak\", exist_ok=True)\n",
    "\n",
    "output_dir_speak = os.path.abspath(\"mixed_up_data_speak_segmented/speak\")\n",
    "output_dir_no_speak = os.path.abspath(\"mixed_up_data_speak_segmented/no_speak\")\n",
    "\n",
    "speak_dir = os.path.abspath(\"mixed_up_data_speak/speak\")\n",
    "no_speak_dir = os.path.abspath(\"mixed_up_data_speak/no_speak\")\n",
    "# ripetiamo per chiarezza\n",
    "\n",
    "music_files = [f for f in os.listdir(speak_dir)]\n",
    "noisy_files = [f for f in os.listdir(no_speak_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5103b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44deb165a4c402db6e776ea84c9f560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finestratura Audio Speak:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2838\n"
     ]
    }
   ],
   "source": [
    "for fname in tqdm(music_files, desc=\"Finestratura Audio Speak\"):\n",
    "    in_path = os.path.join(speak_dir,fname)\n",
    "    try:\n",
    "        segment_file(in_path, output_dir_speak, win_s, 12)\n",
    "    except Exception as e:\n",
    "            print(f\"Errore con {fname}: {e}\")\n",
    "\n",
    "music_files = [f for f in os.listdir(output_dir_speak)]\n",
    "print(len(music_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a570c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2620\n"
     ]
    }
   ],
   "source": [
    "for fname in noisy_files:\n",
    "    in_path = os.path.join(no_speak_dir,fname)\n",
    "    try:\n",
    "        segment_file(in_path, output_dir_no_speak, win_s, random.choice([20,23]))\n",
    "    except Exception as e:\n",
    "            print(f\"Errore con {fname}: {e}\")\n",
    "noisy_files = [f for f in os.listdir(output_dir_no_speak)]\n",
    "print(len(noisy_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f296b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset salvato in datasets/labels_mfcc_speak_segmented.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "music_dir = os.path.abspath(\"mixed_up_data_speak_segmented/speak\")\n",
    "noisy_dir = os.path.abspath(\"mixed_up_data_speak_segmented/no_speak\")\n",
    "\n",
    "music_files = [f for f in os.listdir(music_dir)]\n",
    "noisy_files = [f for f in os.listdir(noisy_dir)]\n",
    "# ripetiamo per chiarezza\n",
    "\n",
    "records = []\n",
    "for f in music_files:\n",
    "    records.append({f\"filepath\": os.path.join(music_dir,f), \"label\": \"music\"})\n",
    "for f in noisy_files:\n",
    "    records.append({\"filepath\": os.path.join(noisy_dir,f), \"label\": \"noisy\"})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "df.to_csv(\"datasets/labels_mfcc_speak_segmented.csv\", index=False)\n",
    "print(\"Dataset salvato in datasets/labels_mfcc_speak_segmented.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
