{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "HOP_LENGTH = 256         \n",
    "N_FFT = 1024          \n",
    "N_MFCC = 13                 \n",
    "\n",
    "def extract_mfcc(y) -> np.ndarray:\n",
    "    \n",
    "    y, _ = librosa.load(y, sr=SAMPLE_RATE)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        center=True)\n",
    "    delta = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    feats = np.vstack([mfcc, delta, delta2])\n",
    "    return aggregate_stats(feats)\n",
    "\n",
    "def aggregate_stats(feats: np.ndarray) -> np.ndarray:\n",
    "    out = []\n",
    "    for row in feats:\n",
    "        vals = np.asarray(row, dtype=np.float32)\n",
    "        out.extend([\n",
    "            np.mean(vals),\n",
    "            np.std(vals),\n",
    "            np.median(vals),\n",
    "            np.max(vals) - np.min(vals)\n",
    "        ])\n",
    "    return np.asarray(out, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbb10407fb64a9a965c98cfdfd5cfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estrazione MFCC da music_files:   0%|          | 0/1888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "in_path_music = os.path.abspath(\"mixed_up_data_no_talk_segmented/music\")\n",
    "\n",
    "music_files = [f for f in os.listdir(in_path_music)]\n",
    "\n",
    "df_music = pd.DataFrame({\n",
    "    \"mfcc_coeff\": [extract_mfcc(os.path.join(in_path_music, f)) for f in tqdm(music_files, desc=\"Estrazione MFCC da music_files\")],\n",
    "    \"label\":      0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b6277929304588a78af285f3acfe84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estrazione MFCC da noise_files:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_path_noise = os.path.abspath(\"mixed_up_data_no_talk/noisy\")\n",
    "noise_files = [f for f in os.listdir(in_path_noise)]\n",
    "\n",
    "df_noisy = pd.DataFrame({\n",
    "    \"mfcc_coeff\": [extract_mfcc(os.path.join(in_path_noise, f)) for f in tqdm(noise_files, desc=\"Estrazione MFCC da noise_files\")],\n",
    "    \"label\":      1\n",
    "})\n",
    "\n",
    "train = pd.concat([df_music, df_noisy], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine dell'addestramento\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=1000)\n",
    "\n",
    "X = np.vstack(train[\"mfcc_coeff\"].values)   # matrice sample - coefficient\n",
    "y = train[\"label\"].values                   # array 1D di etichette\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Fine dell'addestramento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       378\n",
      "           1       1.00      0.96      0.98        23\n",
      "\n",
      "    accuracy                           1.00       401\n",
      "   macro avg       1.00      0.98      0.99       401\n",
      "weighted avg       1.00      1.00      1.00       401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_test/music_pure.wav → music: 1.000, noisy: 0.000\n",
      "audio_test/noise_pure.wav → music: 0.000, noisy: 1.000\n",
      "audio_test/voice_base_music.wav → music: 1.000, noisy: 0.000\n",
      "audio_test/voice_base_pure.wav → music: 1.000, noisy: 0.000\n",
      "audio_test/voice_base_noise.wav → music: 0.906, noisy: 0.094\n"
     ]
    }
   ],
   "source": [
    "def predict_noisy_probability(wav_path):\n",
    "    emb = extract_mfcc(wav_path).reshape(1, -1)\n",
    "    probs = model.predict_proba(emb)[0]   # array di lunghezza 2\n",
    "    return probs\n",
    "\n",
    "test_files = [\n",
    "    \"audio_test/music_pure.wav\",\n",
    "    \"audio_test/noise_pure.wav\",\n",
    "    \"audio_test/voice_base_music.wav\",\n",
    "    \"audio_test/voice_base_pure.wav\",\n",
    "    \"audio_test/voice_base_noise.wav\"\n",
    "]\n",
    "for test_file in test_files:\n",
    "    if os.path.exists(test_file):\n",
    "        p_clean, p_noisy = predict_noisy_probability(test_file)\n",
    "        print(f\"{test_file} → music: {p_clean:.3f}, noisy: {p_noisy:.3f}\")\n",
    "    else:\n",
    "        print(f\"File di test non trovato: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve Descrizione\n",
    "Il modello, che probabilmente overfitta sulla poca mole di dati, in realtà è conferma un qualcosa che conosciamo a priori: i MFCC sono adatti a separare componente di parlato, o meglio, a riconoscere il parlato e anche da diverse sorgenti, tuttavia determinare se l'audio sia musica o meno, in ipotesi in cui non ci sia parlato, allora si dimostra del tutto efficiente in quanto le caratteristiche statistiche della musica sono simili a quelle del parlato. Gli errori che vengono commessi sull'audio complessivo, come music_pure, vengono mitigati usando l'approccio majority, come si può vedere dopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decisione per audio_test/music_pure (majority vote): 0\n",
      "\n",
      "Decisione per audio_test/noise_pure (majority vote): 1\n",
      "\n",
      "Decisione per audio_test/voice_base_music (majority vote): 0\n",
      "\n",
      "Decisione per audio_test/voice_base_pure (majority vote): 0\n",
      "\n",
      "Decisione per audio_test/voice_base_noise (majority vote): 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "WINDOW_SECONDS = 3\n",
    "\n",
    "def aggregate_stats(feats: np.ndarray) -> np.ndarray:\n",
    "  \n",
    "    out = []\n",
    "    for row in feats:\n",
    "        vals = row.astype(np.float32)\n",
    "        out.extend([\n",
    "            np.mean(vals),\n",
    "            np.std(vals),\n",
    "            np.median(vals),\n",
    "            np.max(vals) - np.min(vals)\n",
    "        ])\n",
    "    return np.asarray(out, dtype=np.float32)\n",
    "\n",
    "def extract_mfcc_from_signal(y: np.ndarray) -> np.ndarray:\n",
    "  \n",
    "    mfcc    = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE,\n",
    "                                   n_mfcc=N_MFCC,\n",
    "                                   n_fft=N_FFT,\n",
    "                                   hop_length=HOP_LENGTH,\n",
    "                                   center=True)\n",
    "    delta1  = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2  = librosa.feature.delta(mfcc, order=2)\n",
    "    feats   = np.vstack([mfcc, delta1, delta2])\n",
    "    return aggregate_stats(feats)\n",
    "\n",
    "def segment_audio(path: str, window_sec: float = WINDOW_SECONDS) -> List[np.ndarray]:\n",
    "\n",
    "    y, _ = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    win_len = int(window_sec * SAMPLE_RATE)\n",
    "    n_segs  = int(np.ceil(len(y) / win_len))\n",
    "    segments = []\n",
    "    for i in range(n_segs):\n",
    "        start = i * win_len\n",
    "        end   = start + win_len\n",
    "        seg   = y[start:end]\n",
    "        if len(seg) < win_len:\n",
    "            seg = np.pad(seg, (0, win_len - len(seg)), mode='constant')\n",
    "        segments.append(seg)\n",
    "    return segments\n",
    "\n",
    "def extract_features_per_segment(path: str) -> np.ndarray:\n",
    "\n",
    "    segments = segment_audio(path)\n",
    "    feats = [extract_mfcc_from_signal(seg) for seg in segments]\n",
    "    return np.vstack(feats)\n",
    "\n",
    "def classify_segments(path: str, model: LogisticRegression) -> List[int]:\n",
    "  \n",
    "    X = extract_features_per_segment(path)\n",
    "    return model.predict(X).tolist()\n",
    "\n",
    "def majority_vote(preds: List[int]) -> int:\n",
    "\n",
    "    cnt = Counter(preds)\n",
    "    return cnt.most_common(1)[0][0]\n",
    "\n",
    "def global_decision_majority(path: str, model: LogisticRegression) -> int:\n",
    "\n",
    "    seg_preds = classify_segments(path, model)\n",
    "    return majority_vote(seg_preds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    test_files = [\n",
    "    \"audio_test/music_pure.wav\",\n",
    "    \"audio_test/noise_pure.wav\",\n",
    "    \"audio_test/voice_base_music.wav\",\n",
    "    \"audio_test/voice_base_pure.wav\",\n",
    "    \"audio_test/voice_base_noise.wav\"\n",
    "    ]\n",
    "\n",
    "    for audio_file in test_files:\n",
    "        segment_preds = classify_segments(audio_file, model)\n",
    "\n",
    "        base, _ = os.path.splitext(audio_file)\n",
    "        global_pred = global_decision_majority(audio_file, model)\n",
    "        print(f\"\\nDecisione per {base} (majority vote): {global_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "\n",
    "music = 0\n",
    "noise = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
