{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "from joblib import load\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "HOP_LENGTH = 256         \n",
    "N_FFT = 1024          \n",
    "N_MFCC = 13                 \n",
    "\n",
    "def extract_mfcc(y) -> np.ndarray:\n",
    "    \n",
    "    y, _ = librosa.load(y, sr=SAMPLE_RATE)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        center=True)\n",
    "    delta = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    feats = np.vstack([mfcc, delta, delta2])\n",
    "    return aggregate_stats(feats)\n",
    "\n",
    "def aggregate_stats(feats: np.ndarray) -> np.ndarray:\n",
    "    out = []\n",
    "    for row in feats:\n",
    "        vals = np.asarray(row, dtype=np.float32)\n",
    "        out.extend([\n",
    "            np.mean(vals),\n",
    "            np.std(vals),\n",
    "            np.median(vals),\n",
    "            np.max(vals) - np.min(vals)\n",
    "        ])\n",
    "    return np.asarray(out, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af177a7433c64e079a97194305d698f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estrazione MFCC da speak_files:   0%|          | 0/2838 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "in_path_music = os.path.abspath(\"mixed_up_data_speak_segmented/speak\")\n",
    "\n",
    "speak_files = [f for f in os.listdir(in_path_music)]\n",
    "\n",
    "df_speak = pd.DataFrame({\n",
    "    \"mfcc_coeff\": [extract_mfcc(os.path.join(in_path_music, f)) for f in tqdm(speak_files, desc=\"Estrazione MFCC da speak_files\")],\n",
    "    \"label\":      1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8aece2054494d37a35a11e3b233ea3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estrazione MFCC da no_speak_files:   0%|          | 0/2620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_path_noise = os.path.abspath(\"mixed_up_data_speak_segmented/no_speak\")\n",
    "no_speak_files = [f for f in os.listdir(in_path_noise)]\n",
    "\n",
    "df_no_speak = pd.DataFrame({\n",
    "    \"mfcc_coeff\": [extract_mfcc(os.path.join(in_path_noise, f)) for f in tqdm(no_speak_files, desc=\"Estrazione MFCC da no_speak_files\")],\n",
    "    \"label\":      0\n",
    "})\n",
    "\n",
    "train = pd.concat([df_speak, df_no_speak], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine dell'addestramento\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=1000)\n",
    "\n",
    "X = np.vstack(train[\"mfcc_coeff\"].values) \n",
    "y = train[\"label\"].values                  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Fine dell'addestramento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85       524\n",
      "           1       0.85      0.88      0.87       568\n",
      "\n",
      "    accuracy                           0.86      1092\n",
      "   macro avg       0.86      0.86      0.86      1092\n",
      "weighted avg       0.86      0.86      0.86      1092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_test/music_pure.wav → no_speak: 0.013, speak: 0.987\n",
      "audio_test/noise_pure.wav → no_speak: 0.805, speak: 0.195\n",
      "audio_test/voice_base_music.wav → no_speak: 0.000, speak: 1.000\n",
      "audio_test/voice_base_pure.wav → no_speak: 0.000, speak: 1.000\n",
      "audio_test/voice_base_noise.wav → no_speak: 0.000, speak: 1.000\n"
     ]
    }
   ],
   "source": [
    "def predict_noisy_probability(wav_path):\n",
    "    emb = extract_mfcc(wav_path).reshape(1, -1)\n",
    "    probs = model.predict_proba(emb)[0]   # array di lunghezza 2\n",
    "    return probs\n",
    "\n",
    "test_files = [\n",
    "    \"audio_test/music_pure.wav\",\n",
    "    \"audio_test/noise_pure.wav\",\n",
    "    \"audio_test/voice_base_music.wav\",\n",
    "    \"audio_test/voice_base_pure.wav\",\n",
    "    \"audio_test/voice_base_noise.wav\"\n",
    "]\n",
    "for test_file in test_files:\n",
    "    if os.path.exists(test_file):\n",
    "        p_no_speak, p_speak = predict_noisy_probability(test_file)\n",
    "        print(f\"{test_file} → no_speak: {p_no_speak:.3f}, speak: {p_speak:.3f}\")\n",
    "    else:\n",
    "        print(f\"File di test non trovato: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve Descrizione\n",
    "Otteniamo un F1 - Score dell'85%, il che sembrano risultati ragionevolmente più robusti degli altri casi. La componente di errore sta, probabilmente, nel fatto che come abbiamo visto anche dagli altri esempi le proprietà statistiche su MFCC e Derivatives sono molto simili tra musica e parlato, di conseguenza parte dell'errore sta nel fatto che il modello classifichi come parlato input music - only, ma ciò è perfettamente normale. In realtà ci aspetteremmo metriche ragionevolmente simili anche nel caso del classifier mfcc_talk, tuttavia ricadiamo nello stesso discorso sulla rappresentatività dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello Logistic Regression salvato in classifier_mfcc_speak.joblib …\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['classifier_mfcc_speak.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_OUT = \"classifier_mfcc_speak.joblib\"\n",
    "\n",
    "print(f\"Modello Logistic Regression salvato in {MODEL_OUT} …\")\n",
    "joblib.dump({'model': model}, MODEL_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decisione per audio_test/music_pure (majority vote): 0\n",
      "\n",
      "Decisione per audio_test/noise_pure (majority vote): 0\n",
      "\n",
      "Decisione per audio_test/voice_base_music (majority vote): 1\n",
      "\n",
      "Decisione per audio_test/voice_base_pure (majority vote): 1\n",
      "\n",
      "Decisione per audio_test/voice_base_noise (majority vote): 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "WINDOW_SECONDS = 3\n",
    "\n",
    "def aggregate_stats(feats: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    out = []\n",
    "    for row in feats:\n",
    "        vals = row.astype(np.float32)\n",
    "        out.extend([\n",
    "            np.mean(vals),\n",
    "            np.std(vals),\n",
    "            np.median(vals),\n",
    "            np.max(vals) - np.min(vals)\n",
    "        ])\n",
    "    return np.asarray(out, dtype=np.float32)\n",
    "\n",
    "def extract_mfcc_from_signal(y: np.ndarray) -> np.ndarray:\n",
    "   \n",
    "    mfcc    = librosa.feature.mfcc(y=y, sr=SAMPLE_RATE,\n",
    "                                   n_mfcc=N_MFCC,\n",
    "                                   n_fft=N_FFT,\n",
    "                                   hop_length=HOP_LENGTH,\n",
    "                                   center=True)\n",
    "    delta1  = librosa.feature.delta(mfcc, order=1)\n",
    "    delta2  = librosa.feature.delta(mfcc, order=2)\n",
    "    feats   = np.vstack([mfcc, delta1, delta2])\n",
    "    return aggregate_stats(feats)\n",
    "\n",
    "def segment_audio(path: str, window_sec: float = WINDOW_SECONDS) -> List[np.ndarray]:\n",
    "\n",
    "    y, _ = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    win_len = int(window_sec * SAMPLE_RATE)\n",
    "    n_segs  = int(np.ceil(len(y) / win_len))\n",
    "    segments = []\n",
    "    for i in range(n_segs):\n",
    "        start = i * win_len\n",
    "        end   = start + win_len\n",
    "        seg   = y[start:end]\n",
    "        if len(seg) < win_len:\n",
    "            seg = np.pad(seg, (0, win_len - len(seg)), mode='constant')\n",
    "        segments.append(seg)\n",
    "    return segments\n",
    "\n",
    "def extract_features_per_segment(path: str) -> np.ndarray:\n",
    "\n",
    "    segments = segment_audio(path)\n",
    "    feats = [extract_mfcc_from_signal(seg) for seg in segments]\n",
    "    return np.vstack(feats)\n",
    "\n",
    "def classify_segments(path: str, model: LogisticRegression) -> List[int]:\n",
    "  \n",
    "    X = extract_features_per_segment(path)        \n",
    "    return model.predict(X).tolist()\n",
    "\n",
    "def majority_vote(preds: List[int]) -> int:\n",
    "   \n",
    "    cnt = Counter(preds)\n",
    "    return cnt.most_common(1)[0][0]\n",
    "\n",
    "def global_decision_majority(path: str, model: LogisticRegression) -> int:\n",
    "\n",
    "    seg_preds = classify_segments(path, model)\n",
    "    return majority_vote(seg_preds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # questa logica sarà da modificare su ComfyUI per raccogliere input da IO\n",
    "    test_files = [\n",
    "    \"audio_test/music_pure.wav\",\n",
    "    \"audio_test/noise_pure.wav\",\n",
    "    \"audio_test/voice_base_music.wav\",\n",
    "    \"audio_test/voice_base_pure.wav\",\n",
    "    \"audio_test/voice_base_noise.wav\"\n",
    "    ]\n",
    "    \n",
    "    data = load(MODEL_OUT)\n",
    "    model  = data['model']\n",
    "\n",
    "    for audio_file in test_files:\n",
    "        segment_preds = classify_segments(audio_file, model)\n",
    "\n",
    "        base, _ = os.path.splitext(audio_file)\n",
    "        global_pred = global_decision_majority(audio_file, model)\n",
    "        print(f\"\\nDecisione per {base} (majority vote): {global_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "\n",
    "voice = 1\n",
    "no_voice = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
