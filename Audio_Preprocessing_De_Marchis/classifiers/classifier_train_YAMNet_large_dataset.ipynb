{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91e99b2",
   "metadata": {},
   "source": [
    "# Breve Descrizione\n",
    "Questo è un modello con ragionamenti praticamente analoghi all'altro classificatore con YAMnet ma allenato su un dataset molto più grande, costruito iterando le procedure per 'mixed_up_data_talk' su circa 200k record, abbastanza complesso da non far overfittare YAMNet. I risultati sono, siffatti, assolutamente più plausibili e, poi, è quello che abbiamo utilizzato nello scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6a92a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Forza utilizzo della CPU per il train, più 'sbrigativo' che CUDA\u001b[39;00m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Forza utilizzo della CPU per il train, più 'sbrigativo' che CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2647d450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domenicodemarchis/Desktop/IAARTS/classifier/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH   = \"datasets/labels.csv\"\n",
    "MODEL_OUT  = \"yamnet_model_large_dataset.joblib\"      # Salvo nella directory corrente, si può cambiare percorso\n",
    "SR         = 16000                      \n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "label_map = {\"music\": 0, \"noisy\": 1}\n",
    "df[\"label_num\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "file_label_list = list(zip(df[\"filepath\"].tolist(),\n",
    "                           df[\"label_num\"].tolist()))\n",
    "\n",
    "yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "def extract_yamnet_embedding(wav_path, sr=16000):\n",
    "    wav, _ = librosa.load(wav_path, sr=sr, mono=True)\n",
    "\n",
    "    wav_tf = tf.convert_to_tensor(wav, dtype=tf.float32)\n",
    "\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_tf)\n",
    "\n",
    "    emb = tf.reduce_mean(embeddings, axis=0)\n",
    "    return emb.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b2a5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for path, label in file_label_list:\n",
    "    emb = extract_yamnet_embedding(path)\n",
    "    X.append(emb)\n",
    "    y.append(label)\n",
    "X = np.vstack(X)\n",
    "y = np.array(y)\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "norm = scaler.fit_transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_reduced = lda.fit_transform(norm,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40314df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valutazione su Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       music       0.75      0.62      0.68       706\n",
      "       noisy       0.63      0.76      0.69       600\n",
      "\n",
      "    accuracy                           0.69      1306\n",
      "   macro avg       0.69      0.69      0.69      1306\n",
      "weighted avg       0.70      0.69      0.69      1306\n",
      "\n",
      "Matrice di Confusione:\n",
      " [[441 265]\n",
      " [146 454]]\n",
      "Modello YAMNet salvato in yamnet_head.joblib …\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(\n",
    "    max_iter=500,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Valutazione su Test Set:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['music','noisy']))\n",
    "print(\"Matrice di Confusione:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(f\"Modello YAMNet salvato in {MODEL_OUT} …\")\n",
    "joblib.dump({'pca': lda, 'clf': clf}, MODEL_OUT)\n",
    "\n",
    "def predict_noisy_probability(wav_path):\n",
    "\n",
    "    emb = extract_yamnet_embedding(wav_path).reshape(1, -1)\n",
    "    emb_scaled = scaler.transform(emb)         \n",
    "    emb_lda    = lda.transform(emb_scaled)    \n",
    "    prob_noisy = clf.predict_proba(emb_lda)[0, 1]\n",
    "    return prob_noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c4f089",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m test_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_test/voice_base_music.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_test/voice_base_pure.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_test/voice_base_noise.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     ]\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myamnet_head_first.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m pca \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m clf \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "test_files = [\n",
    "    \"audio_test/voice_base_music.wav\",\n",
    "    \"audio_test/voice_base_pure.wav\",\n",
    "    \"audio_test/voice_base_noise.wav\"\n",
    "    ]\n",
    "\n",
    "data = load(\"yamnet_model_large_dataset.joblib\")\n",
    "pca = data['pca']\n",
    "clf = data['clf']\n",
    "\n",
    "for audio_file in test_files:\n",
    "    waveform, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
    "\n",
    "    yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "    _, embeddings, _ = yamnet(waveform)\n",
    "    feat = np.mean(embeddings.numpy(), axis=0).reshape(1, -1)\n",
    "\n",
    "    feat_pca = pca.transform(feat)\n",
    "\n",
    "    pred_label = clf.predict(feat_pca)\n",
    "    pred_proba = clf.predict_proba(feat_pca)\n",
    "\n",
    "    base, _ = os.path.splitext(audio_file)\n",
    "    print(f\"Predizione per {base}:\", pred_label[0])\n",
    "    print(f\"Probabilità per {base}:\", pred_proba[0])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
